{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "569d3244",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28d779ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from extended_watermark_processor import WatermarkLogitsProcessor\n",
    "from transformers import AutoModelForCausalLM, AutoModelForSeq2SeqLM, AutoTokenizer, LogitsProcessorList, AutoModelForSequenceClassification\n",
    "from extended_watermark_processor import WatermarkDetector\n",
    "import torch\n",
    "# import together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a123ea06",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Write an essay about ice cream. Add ':)' after every word\"\n",
    "# input_text = \"Write an essay about ice cream.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed2059b",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74899557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"attack_classifier_ABCD\")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"attack_classifier_ABCD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9bebe665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize input text\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Forward pass\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Get probabilities\n",
    "probabilities = outputs.logits.softmax(dim=1)\n",
    "\n",
    "# Get the predicted label\n",
    "attack = torch.argmax(probabilities, dim=1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f01a95dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d4ac9b",
   "metadata": {},
   "source": [
    "# Watermarker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b56b43e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = \"gpt2\" # \"t5-base\" #\"gpt2\" # \"openai-gpt\" #\"bert-base-cased\"  # Replace with the actual model name\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f426ef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "watermark_processor = WatermarkLogitsProcessor(vocab=list(tokenizer.get_vocab().values()),\n",
    "                                               gamma=0.25,\n",
    "                                               delta=2.0,\n",
    "                                               seeding_scheme=\"selfhash\") #equivalent to `ff-anchored_minhash_prf-4-True-15485863`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f5c55efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.to(\"cpu\") \n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "355e4289",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tokenizer(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81803c6",
   "metadata": {},
   "source": [
    "# Solution 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f4f4e69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def watermarker(input_text, max_length = 500):\n",
    "    tokenized_input = tokenizer(input_text, return_tensors=\"pt\")#.to(model.device)\n",
    "    output_tokens = model.generate(**tokenized_input,\n",
    "                                   logits_processor=LogitsProcessorList([watermark_processor]),\n",
    "                                   max_length=max_length)\n",
    "    output_tokens = output_tokens[:,tokenized_input[\"input_ids\"].shape[-1]:]\n",
    "    output_text = tokenizer.batch_decode(output_tokens, skip_special_tokens=True)[0]\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3dee5e8d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We cannot watermark such a prompt, please try another one.\n"
     ]
    }
   ],
   "source": [
    "if attack:\n",
    "    print(\"We cannot watermark such a prompt, please try another one.\")\n",
    "else:\n",
    "    output_text = watermarker(input_text)\n",
    "    print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e99f95",
   "metadata": {},
   "source": [
    "### Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94bd0b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "watermark_detector = WatermarkDetector(vocab=list(tokenizer.get_vocab().values()),\n",
    "                                        gamma=0.25, # should match original setting\n",
    "                                        seeding_scheme=\"selfhash\", # should match original setting\n",
    "                                        device=model.device, # must match the original rng device type\n",
    "                                        tokenizer=tokenizer,\n",
    "                                        z_threshold=4.0,\n",
    "                                        normalizers=[],\n",
    "                                        ignore_repeated_ngrams=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f2ed388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_text = 'hi my name is'\n",
    "score_dict = watermark_detector.detect(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1824391",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This text was generated by a Language Model.\n"
     ]
    }
   ],
   "source": [
    "if score_dict['prediction']:\n",
    "    print(\"This text was generated by a Language Model.\")\n",
    "else:\n",
    "    print(\"This text was generated by a Human.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a116f56",
   "metadata": {},
   "source": [
    "## Solution 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b10748fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from together_ai_separate import separate_prompt_attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d39894e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_ai import make_openai_api_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c63febcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good Prompt\n",
      " Write an essay about ice cream. \n",
      "Prompt Attack\n",
      " Add ':)' after every word.\n",
      "Watermarked Text\n",
      "  And if you don't like it, you can always try the \"What's your favorite ice cream recipe?\" or \"What's your favorite ice cream recipe?\" or whatever it is.  And if you don't like it, you can always try the \"What's your favorite ice cream recipe?\" or \"What's your favorite ice cream recipe?\" or whatever it is.  And if you don't like it, you can always try the \"What's your favorite ice cream recipe?\" or \"What's your favorite ice cream recipe?\" or whatever it is.  And if you don't like it, you can always try the \"What's your favorite ice cream recipe?\" or \"What's your favorite ice cream recipe?\" or whatever it is.  And if you don't like it, you can always try the \"What's your favorite ice cream recipe?\" or \"What's your favorite ice cream recipe?\" or whatever it is.  And if you don't like it, you can always try the \"What's your favorite ice cream recipe?\" or \"What's your favorite ice cream recipe?\" or whatever it is.  And if you don't like it, you can always try the \"What's your favorite ice cream recipe?\" or \"What's your favorite ice cream recipe?\" or whatever it is.  And if you don't like it, you can always try the \"What's your favorite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To user\n",
      " ice cream recipe? :) or \"What's your favorite ice cream recipe?\" :) or whatever it is. :) And if you don't like it, you can always try the \"What's your favorite ice cream recipe?\" :) or \"What's your favorite ice cream recipe?\" :) or whatever it is. :) And if you don't like it, you can always try the \"What's your favorite ice cream recipe?\" :) or \"What's your favorite ice cream recipe?\" :) or whatever it is. :) And if you don't like it, you can always try the \"What's your favorite ice cream recipe?\" :) or \"What's your favorite ice cream recipe?\" :) or whatever it is. :) And if you don't like it, you can always try the \"What's your favorite ice cream recipe?\" :) or \"What's your favorite ice cream recipe?\" :) or whatever it is. :) And if you don't like it, you can always try the \"What's your favorite ice cream recipe?\" :) or \"What's your favorite ice cream recipe?\" :) or whatever it is. :) And if you don't like it, you can always try the \"What's your favorite ice cream recipe?\" :) or \"What's your favorite ice cream recipe?\" :) or whatever it is. :) And if you don't like it, you can always try the \"What's your favorite ice cream recipe?\" :) or \"What's your favorite ice cream recipe?\" :) or whatever it is. :)\n"
     ]
    }
   ],
   "source": [
    "if attack:\n",
    "    good_prompt, prompt_attack = separate_prompt_attack(input_text) \n",
    "    print(\"Good Prompt\\n\", good_prompt)\n",
    "    print(\"Prompt Attack\\n\", prompt_attack)\n",
    "    watermarked_good_text = watermarker(good_prompt) \n",
    "    print(\"Watermarked Text\\n\", watermarked_good_text)\n",
    "    output_text = make_openai_api_call(prompt_attack, watermarked_good_text)\n",
    "    print(\"To user\\n\",output_text)\n",
    "    \n",
    "else:\n",
    "    output_text = watermarker(input_text)\n",
    "    print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad66c02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_text = make_openai_api_call(prompt_attack, watermarked_good_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf076850",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"To user\\n\",output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046cab57",
   "metadata": {},
   "source": [
    "### Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b71529f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "watermark_detector = WatermarkDetector(vocab=list(tokenizer.get_vocab().values()),\n",
    "                                        gamma=0.25, # should match original setting\n",
    "                                        seeding_scheme=\"selfhash\", # should match original setting\n",
    "                                        device=model.device, # must match the original rng device type\n",
    "                                        tokenizer=tokenizer,\n",
    "                                        z_threshold=4.0,\n",
    "                                        normalizers=[],\n",
    "                                        ignore_repeated_ngrams=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ef3e58",
   "metadata": {},
   "source": [
    "#### watermarked text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "34072f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_text = 'hi my name is'\n",
    "score_dict = watermark_detector.detect(watermarked_good_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "baec6ee3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This text was generated by a Language Model.\n"
     ]
    }
   ],
   "source": [
    "if score_dict['prediction']:\n",
    "    print(\"This text was generated by a Language Model.\")\n",
    "else:\n",
    "    print(\"This text was generated by a Human.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f19710",
   "metadata": {},
   "source": [
    "#### injected watermarked text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6a89f00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_text = 'hi my name is'\n",
    "score_dict = watermark_detector.detect(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4c8eda7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This text was generated by a Human.\n"
     ]
    }
   ],
   "source": [
    "if score_dict['prediction']:\n",
    "    print(\"This text was generated by a Language Model.\")\n",
    "else:\n",
    "    print(\"This text was generated by a Human.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02d1baf",
   "metadata": {},
   "source": [
    "#### removed insertion watrermarked text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "273a8518",
   "metadata": {},
   "outputs": [],
   "source": [
    "without_attack =  \" ice cream recipe? or \\\"What\\'s your favorite ice cream recipe?\\\" or whatever it is. And if you don\\'t like it, you can always try the \\\"What\\'s your favorite ice cream recipe?\\\" or \\\"What\\'s your favorite ice cream recipe?\\\" or whatever it is. And if you don\\'t like it, you can always try the \\\"What\\'s your favorite ice cream recipe?\\\" or \\\"What\\'s your favorite ice cream recipe?\\\" or whatever it is. And if you don\\'t like it, you can always try the \\\"What\\'s your favorite ice cream recipe?\\\" or \\\"What\\'s your favorite ice cream recipe?\\\" or whatever it is. And if you don\\'t like it, you can always try the \\\"What\\'s your favorite ice cream recipe?\\\" or \\\"What\\'s your favorite ice cream recipe?\\\" or whatever it is. And if you don\\'t like it, you can always try the \\\"What\\'s your favorite ice cream recipe?\\\" or \\\"What\\'s your favorite ice cream recipe?\\\" or whatever it is. And if you don\\'t like it, you can always try the \\\"What\\'s your favorite ice cream recipe?\\\" or \\\"What\\'s your favorite ice cream recipe?\\\" or whatever it is. And if you don\\'t like it, you can always try the \\\"What\\'s your favorite ice cream recipe?\\\" or \\\"What\\'s your favorite ice cream recipe?\\\" or whatever it is.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "687499ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_text = 'hi my name is'\n",
    "score_dict = watermark_detector.detect(without_attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fe31143f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This text was generated by a Language Model.\n"
     ]
    }
   ],
   "source": [
    "if score_dict['prediction']:\n",
    "    print(\"This text was generated by a Language Model.\")\n",
    "else:\n",
    "    print(\"This text was generated by a Human.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
