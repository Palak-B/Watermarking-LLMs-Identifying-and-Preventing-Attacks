from extended_watermark_processor import WatermarkLogitsProcessor
from transformers import AutoModelForCausalLM, AutoModelForSeq2SeqLM, AutoTokenizer, LogitsProcessorList, AutoModelForSequenceClassification
from extended_watermark_processor import WatermarkDetector
import torch


def classify_attack(input_text):
    model = AutoModelForSequenceClassification.from_pretrained("attack_classifier_ABCD")

    # Load the tokenizer
    tokenizer = AutoTokenizer.from_pretrained("attack_classifier_ABCD")

    # Tokenize input text
    inputs = tokenizer(input_text, return_tensors="pt")

    # Forward pass
    outputs = model(**inputs)

    # Get probabilities
    probabilities = outputs.logits.softmax(dim=1)

    # Get the predicted label
    attack = torch.argmax(probabilities, dim=1).item()

    return attack

def watermarker(input_text, max_length = 500):
    model_name = "gpt2"
    model = AutoModelForCausalLM.from_pretrained(model_name)
    # model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    watermark_processor = WatermarkLogitsProcessor(vocab=list(tokenizer.get_vocab().values()),
                                                gamma=0.25,
                                                delta=2.0,
                                                seeding_scheme="selfhash") #equivalent to `ff-anchored_minhash_prf-4-True-15485863`

    tokenized_input = tokenizer(input_text, return_tensors="pt")#.to(model.device)
    output_tokens = model.generate(**tokenized_input,
                                logits_processor=LogitsProcessorList([watermark_processor]),
                                max_length=max_length)
    output_tokens = output_tokens[:,tokenized_input["input_ids"].shape[-1]:]
    output_text = tokenizer.batch_decode(output_tokens, skip_special_tokens=True)[0]
    return output_text

def detector_function(output_text):
    model_name = "gpt2"
    model = AutoModelForCausalLM.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    watermark_detector = WatermarkDetector(vocab=list(tokenizer.get_vocab().values()),
                                            gamma=0.25, # should match original setting
                                            seeding_scheme="selfhash", # should match original setting
                                            device=model.device, # must match the original rng device type
                                            tokenizer=tokenizer,
                                            z_threshold=4.0,
                                            normalizers=[],
                                            ignore_repeated_ngrams=True)

    score_dict = watermark_detector.detect(output_text)

    if score_dict['prediction']:
        return "This text was generated by a Language Model."
    else:
        return "This text was generated by a Human."